{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8-1-합성곱_신경망의_구성_요소.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO2CJs01r22a2C6hd37uB0v"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 합성곱\n",
        "예를 들어 입력 배열 [3, 1, 0, 7, 6, 4, 8, 2, 4, 5]가 있다고 하자. 7장에서 사용한 밀집층에는 모든 입력에 가중치를 곱한다. \n",
        "$$3 \\times w_1 + 1 \\times w_2 + 0 \\times w_3 + 7 \\times w_4 +\n",
        "  6 \\times w_5 + 4 \\times w_6 + 8 \\times w_7 + 2 \\times w_8 + \n",
        "  4 \\times w_9 + 5 \\times w_10 + b$$\n",
        "인공 신경망은 처음에 가중치 w1 ~ w10과 절편 b를 랜덤하게 초기화한 다음 에폭을 반복하며 경사 하강법 알고리즘을 사용하여 손실이 낮아지도록 최적의 가중치와 절편을 찾아간다. 반면 **합성곱(convolution)**은 계산이 다르다. 입력 데이터 일부에 가중치를 더한다. 입력은 위 배열과 같다고 하고, 각 뉴런에 3개의 가중치가 있다고 하자. 그럼 처음에는 다음과 같이 계산된다\n",
        "$$3 \\times w_1 + 1 \\times w_2 + 0 \\times w_3 + b$$\n",
        "그 다음은 한 칸씩만 밀어서 조금 겹쳐지게 계산된다\n",
        "$$1 \\times w_1 + 0 \\times w_2 + 7 \\times w_3 + b$$\n",
        "마지막으로 다음과 같이 계산된다\n",
        "$$2 \\times w_1 + 4 \\times w_2 + 5 \\times w_3 + b$$\n",
        "이 식들이 각각 하나의 출력이 된다. 이런 식으로 입력이 10개일 때 뉴런의 가중치가 3개이면 8개의 출력이 만들어진다. 합성곱 층의 뉴런의 가중치 개수는 하이퍼파라미터다. **합성곱 신경망(convolutional neural network, CNN)**은 완전 연결 신경망과 달리 뉴런을 **필터(filter)** 혹은 **커널(kernel)**이라고 부른다. 여기서는 뉴런 개수를 이야기할 때는 필터, 입력에 곱해지는 가중치를 의미할 때는 커널이라 부른다.\n",
        "$$ \\begin{pmatrix}\n",
        "  3 & 1 & 0 & 7 \\\\\n",
        "  6 & 4 & 8 & 2 \\\\\n",
        "  4 & 5 & 1 & 1 \\\\\n",
        "  3 & 2 & 5 & 8 \\\\\n",
        " \\end{pmatrix}$$\n",
        " 이렇게 입력이 2차원일 때는 필터도 2차원이어야 한다. 여기서 필터의 커널 크기는 (3, 3)이라 하자. 그럼 합성곱을 다음과 같이 시작한다.\n",
        " $$ \\begin{pmatrix}\n",
        "  _3 & _1 & _0 & 7 \\\\\n",
        "  _6 & _4 & _8 & 2 \\\\\n",
        "  _4 & _5 & _1 & 1 \\\\\n",
        "  3 & 2 & 5 & 8 \\\\\n",
        " \\end{pmatrix}$$\n",
        " 입력의 9개 원소와 커널의 9개 원소를 곱하고 절편을 더해 1개의 출력을 만든다.\n",
        " 이런 식으로 계속 진행한다.\n",
        " $$ \\begin{pmatrix}\n",
        "  3 & _1 & _0 & _7 \\\\\n",
        "  6 & _4 & _8 & _2 \\\\\n",
        "  4 & _5 & _1 & _1 \\\\\n",
        "  3 & 2 & 5 & 8 \\\\\n",
        " \\end{pmatrix}$$.\n",
        " $$ \\begin{pmatrix}\n",
        "  3 & 1 & 0 & 7 \\\\\n",
        "  _6 & _4 & _8 & 2 \\\\\n",
        "  _4 & _5 & _1 & 1 \\\\\n",
        "  _3 & _2 & _5 & 8 \\\\\n",
        " \\end{pmatrix}$$.\n",
        " $$ \\begin{pmatrix}\n",
        "  3 & 1 & 0 & 7 \\\\\n",
        "  6 & _4 & _8 & _2 \\\\\n",
        "  4 & _5 & _1 & _1 \\\\\n",
        "  3 & _2 & _5 & _8 \\\\\n",
        " \\end{pmatrix}$$\n",
        " 위 처럼 필터가 4번 움직일 수 있으므로 출력은 (2, 2) 크기가 된다. 합성곱 계산을 통해 얻은 출력을 특별히 **특성 맵(feature map)**이라 한다. 여기에서는 하나의 (3, 3) 필터를 사용했기 때문에 (2, 2, 1) 크기의 출력이 생성되었지만, 같은 크기의 필터 3개를 이용하면 (2, 2, 3) 크기의 3차원 출력이 생성된다\n",
        "\n",
        " ## 케라스 합성곱 층\n",
        " 케라스의 층은 모두 keras.layers 패키지 아래 클래스로 구현되어있고, 합성곱은 Conv2D 클래스로 제공한다\n",
        " ``` python\n",
        " from tensorflow import keras\n",
        " keras.layers.Conv2D(10, kernel_size=(3, 3), activation='relu')\n",
        " ```\n",
        " Conv2D 클래스의 첫 번째 매개변수는 필터의 개수다. kernel_size 매개변수는 필터에 사용할 커널의 크기를 지정한다. 마지막으로 활성화 함수를 적용한다\n",
        "\n",
        " ### 패딩과 스트라이드\n",
        " 위에서 예로 들었던 합성곱 계산은 (4, 4) 크기의 입력에 (3, 3) 크기의 커널을 적용하여 (2, 2) 크기의 특성 맵을 만들었다. 커널 크기를 그대로 두고 출력 크기를 (4, 4)로 만들려면 **패딩(padding)**을 이용할 수 있다. 패딩은 입력값 주위에 가상의 원소로 채우는 것을 말하는데 (4, 4) 크기의 입력에 0을 1개 패딩하면 다음과 같이 (6, 6) 크기의 입력이 된다.\n",
        " $$ \\begin{pmatrix}\n",
        "  0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "  0 & 3 & 1 & 0 & 7 & 0 \\\\\n",
        "  0 & 6 & 4 & 8 & 2 & 0 \\\\\n",
        "  0 & 4 & 5 & 1 & 1 & 0 \\\\\n",
        "  0 & 3 & 2 & 5 & 8 & 0 \\\\\n",
        "  0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        " \\end{pmatrix}$$\n",
        " 위 입력을 위 커널로 계산하면 입력과 동일한 크기의 출력이 생성된다. 이렇게 입력과 특성 맵의 크기를 동일하게 하기 위해 0을 패딩하는 것을 **세임 패딩(same padding)**이라 한다. 또한 패딩 없이 입력 배열만 가지고 특성 맵을 만드는 경우를 **밸리드 패딩(valid padding)**이라 한다\n",
        "\n",
        " 합성곱에서 패딩을 자주 사용하는 이유는 다음과 같다. 만약 패딩이 없다면 위의 예에서 (4, 4) 크기의 입력에 (3, 3) 커널로 합성곱을 진행한다면 각 모서리의 값은 단 한 번씩만 사용된다. 이때 입력을 이미지라고 하면 모서리에 있는 중요한 정보가 특성 맵으로 잘 전달되지 않고, 중앙부 정보가 두드러지게 표현될 수 있다. 그러나 패딩을 하게 되면 모서리 부분과 중앙부의 합성곱에 참여하는 비율의 차가 줄어들게 되면서 이미지 주변에 있는 정보를 잃어버리지 않도록 도와준다\n",
        "\n",
        " 위에서 살펴본 합성곱에서의 커널은 상하좌우로 1칸씩 이동하였는데, 이를 2칸 이상씩 이동시킬 수 있다. 이런 이동의 크기를 **스트라이드(stride)**라고 한다. \n",
        "\n",
        " ```python\n",
        " keras.layers.Conv2D(10, kernel_size=(3, 3), activation='relu',\n",
        "                     padding='same', strides=1)\n",
        "```\n",
        "\n",
        "### 풀링\n",
        "**풀링(pooling)**은 합성곱 층에서 만든 특성 맵의 가로세로 크기를 줄이는 역할을 한다. 이때 특성 맵의 개수는 줄이지 않는다. (4, 4) 크기의 입력을 (3, 3) 크기의 커널 3개로 합성곱 하여 나온 출력값 (2, 2, 3)을 (2, 2) 크기로 풀링한다면 (1, 1, 3) 크기의 출력값을 낼 수 있다. 풀링에서는 가중치가 없고 커널로 찍은 영역에서 가장 큰 값을 고르는 **최대 풀링(max pooling)**과 평균값을 계산하는 **평균 풀링(average pooling)**이 있다. 맥스 풀링의 예를 들면 다음과 같다\n",
        "\\begin{pmatrix}\n",
        " 2 & 5 & _7 & 3 \\\\\n",
        " 3 & _9 & 0 & 5 \\\\\n",
        " 6 & 2 & 1 & 4 \\\\\n",
        " 4 & _8 & _6 & 0 \\\\\n",
        "\\end{pmatrix}\n",
        "여기에서 (2, 2) 크기로 맥스 풀링을 한다면\n",
        "\\begin{pmatrix}\n",
        " 9 & 7 \\\\\n",
        " 8 & 6 \\\\\n",
        "\\end{pmatrix}\n",
        "결과값이 나온다. 풀링의 특징은 커널이 겹치지 않는다는 점이다. 커널의 크기가 2이므로 스트라이드가 2가 되었고, 크기가 3이라면 스트라이드는 3이 된다. 케라스에서는 MaxPooling2D로 풀링을 수행할 수 있다\n",
        "```python\n",
        "keras.layers.MaxPooling2D(2, strides=2, padding='valid')\n",
        "```\n",
        "첫 매개변수로 풀링의 크기를 지정한다. 첫 매개변수로 튜플을 전달하면 커널의 가로 세로 크기를 달리 할 수 있는데 이런 경우는 극 ㅡ 히 드물다. strides 매개변수에 스트라이드 크기를 지정할 수 있는데 이는 자동으로 커널 크기로 되므로 거의 놔둔다. padding 매개변수로 패딩을 지정할 수 있는데 보통 'valid'로 패딩을 하지 않는다. 에버리지 풀링은 AveragePooling2D로 제공하는데, 많은 경우에 에버리지는 평균하며 특성 맵에 있는 중요 정보를 희석시킬 수 있어 맥스 풀링을 자주 사용한다\n",
        "\n",
        "## 합성곱 신경망의 전체 구조\n",
        "먼저 (4, 4) 크기의 입력이 들어온다. 이 입력에 세임 패딩을 적용시킨 후 (3, 3) 크기의 필터 3개로 합성곱을 수행한다. 3개의 필터가 하나씩 합성곱의 출력을 만들고 이 출력이 합쳐져서 (4, 4, 3) 크기의 특성 맵이 만들어진다. 합성곱 층에서도 활성화 함수를 적용시키는데, 대부분 렐루 함수를 사용한다. 다음은 풀링층이다. (2, 2) 크기의 커널로 풀링을 진행하여 (2, 2, 3) 크기의 특성 맵을 만든다. 풀링의 사용 이유는 합성곱에서 스트라이드를 크게 하여 특성 맵을 줄이는 것보다 풀링층에서 크기를 줄이는 것이 경험적으로 더 나은 성능을 내기 때문이다. 마지막 밀집층인 출력층에 전달할 때는 Flatten 클래스를 사용하여 3차원 배열을 1차원으로 축소시켜 준다. 출력층에는 n개의 뉴런이 있는데 n의 개수가 곧 분류 클래스의 개수다. 출력층에서 계산된 값은 소프트맥스 활성화 함수를 거쳐 최종 예측 확률이 된다\n",
        "\n",
        "### 컬러 이미지를 사용한 합성곱\n",
        "현재까지는 입력을 2차원 배열로 가정했다. 만약 이미지가 흑백이 아닌 컬러라면 RGB 특성 때문에 3차원으로 표시해야한다. (4, 4) 크기의 입력이 있다면 사실은 (4, 4, 3)인 것이다. 이런 깊이(채널)가 있는 입력에는 깊이가 있는 커널이 필요하다. (3, 3) 크기의 커널이 아닌 (3, 3, 3) 크기의 커널이 필요하다. 이 커널로 합성곱을 진행하면 (2, 2) 크기의 출력이 나오게 된다. 사실 흑백 이미지에도 2차원이 아니라 3차원 입력을 기대한다. (4, 4)가 아닌 (4, 4, 1)을 기대하는 것이다. 깊이가 있는 커널은 컬러 이미지 외에 합성곱 ㅡ 풀링 층 뒤에 다시 합성곱이 올 때도 필요하다. 예를 들어 합성곱 필터를 5개를 사용해 출력의 크기가 (4, 4, 5)인 출력에 다시 합성곱을 할 때는 깊이가 5인 커널이 필요하다. 보통 CNN은 너비와 높이가 점점 줄어들고 깊이는 점점 깊어지는 것이 특징이다. 이렇게 한 후 마지막에 출력층 전에 특성 맵을 모두 펼쳐서 밀집층의 입력으로 사용한다\n",
        "\n",
        "CNN에서 필터는 이미지에 있는 어떤 특징을 찾는다고 생각할 수 있다. 또한 어떤 특징이 이미지의 어느 위치에 놓이더라도 쉽게 감지할 수 있도록 너비와 높이 차원을 점점 줄여가는 것이다"
      ],
      "metadata": {
        "id": "-Hx-5TVbfHrJ"
      }
    }
  ]
}