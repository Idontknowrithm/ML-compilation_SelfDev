{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4-2-확률적_경사_하강법.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyODzIVPZMHmqepTcGLVCXFB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBjca8mjF0fr"
      },
      "source": [
        "## 점진적인 학습\n",
        "만약 훈련 데이터가 한 번에 준비되는 것이 아니라 조금씩 전달된다고 할 때, 어떤 데이터를 소실시키지 않고 새로운 데이터에 대해서만 조금씩 더 훈련하는 방식을 **점진적 학습(incremental learning)** 혹은 온라인 학습이라 한다. 대표적인 점진적 학습 알고리즘으로 **확률적 경사 하강법(Stochastic Gradient Descent)**이 있다\n",
        "\n",
        "### 확률적 경사 하강법\n",
        "언덕에서 공을 굴린다고 생각해봤을 때, 가장 빠르게 내려오는 방법은 가장 가파른 지형을 따라 내려오는 것이다. 확률적 경사 하강법(SGD)은 이런 식으로, 가장 가파른 경사를 따라 목표 지점에 도달하는 것이다. 이때 주의해야할 점은 한 번에 긴 거리를 가게 된다면 목표 지점인 부분을 넘어 다시 위로 올라갈 수 있다(가속도가 있다면 한 번에 멈추기 어려운 점을 떠올리자). 따라서 경사를 내려올 때는 가파른 경사를 따라 조금씩 내려오는 것이 중요하고, 이렇게 내려오는 과정이 SGD 모델을 훈련하는 과정이다\n",
        "\n",
        "SGD에서 확률적이라는 의미는, 경사 하강법을 통해 가장 가파른 길을 찾을 때 전체 샘플을 사용하지 않고 딱 하나의 샘플을 훈련셋에서 랜덤하게 골라 가장 가파른 길을 찾는다는 것이다. 훈련셋에서 랜덤하게 하나의 샘플을 선택해 가파른 경사를 조금 내려가고, 이런 식으로 해서 전체 샘플을 모두 사용한다. 이때 목표 지점에 도달하지 못했다면? 다시 훈련셋을 모두 사용해서 전에 했던 훈련을 반복하는데, 여기서 훈련셋을 한 번 모두 사용하는 과정을 **에폭(epoch)**이라고 부른다\n",
        "\n",
        "SGD에서처럼 하나씩 훈련셋에서 뽑아 훈련하지 않고, 몇 개의 샘플을 동시에 사용해서 경사 하강법을 수행하면 **미니배치 경사 하강법(minibatch gradient descent)**이라 하고, 전체 샘플을 사용하면 **배치 경사 하강법(batch gradient descent)**이라 한다\n",
        "\n",
        "### 손실 함수\n",
        "빠르게 내려오려고하는 언덕 자체를 손실 함수라 할 수 있다. **손실함수(loss function)**는 어떤 문제에서 머신러닝 알고리즘의 예측 실패 정도를 측정하는 기준이다. 손실 함수의 최소값은 없으나 손실 함수의 값이 작을 수록 좋다\n",
        "\n",
        "분류에서의 손실은 오답을 예측하는 것이다. 한 훈련셋에 대한 예측이 다음과 같다면\n",
        "1. 예측: 1, 정답: 1\n",
        "2. 예측: 0, 정답: 1\n",
        "3. 예측: 0, 정답: 0\n",
        "4. 예측: 1, 정답: 0\n",
        "\n",
        "정확도는 2/4 = 0.5다. 만약 정확도에 음수를 취한 값을 손실 함수로 잡는다면, -1이 가장 낮고, -0이 가장 높은 손실 함수가 될 수 있다. 그러나 여기에서 문제는 정확도는 미분 불가능하다는 점이다. 앞에서처럼 4개의 샘플이 있다면 가능한 손실 함수의 값은 0, 0.25, 0.5, 0.75, 1로 5개밖에 없다. 경사 하강법에서 아주 조금씩 내려오는 것을 구현하려면 손실 함수는 미분 가능해야한다. 이렇게 연속적이고 미분 가능한 손실 함수를 만들기 위해서 로지스틱 손실함수를 사용할 수 있다.\n",
        "\n",
        "### 로지스틱 손실 함수\n",
        "위의 샘플 4개의 예측 성공 확률을 0.9, 0.3, 0.2, 0.8이라 하자. \n",
        "\n",
        "첫 번째 샘플 예측은 0.9이고, 정답이 양성 클래스(1)므로 -(1 * 0.9) = -0.9가 손실 함수가 된다. \n",
        "\n",
        "두 번째 샘플 예측은 0.3이고, 정답이 양성 클래스이므로 -(1 * 0.3) = -0.3이 손실 함수가 된다. \n",
        "\n",
        "세 번째 샘플 예측은 0.2인데, 정답이 음성 클래스이므로 그대로 0 * 0.2가 되면 무조건 0이 되므로 타깃을 양성 클래스로 바꾸고, 예측값도 양성 클래스에 대한 예측으로 바꾼다. 즉 1 - 0.2 = 0.8로 만들고, 이를 손실함수로 바꾸면 -(1 * 0.8) = -0.8이 손실 함수가 된다.\n",
        "\n",
        "네 번째 샘플 예측은 0.8이고, 타깃이 음성 클래스이므로 세 번째 샘플과 동일한 방법을 취한다. 따라서 -(1 * 0.2) = -0.2가 손실 함수가 된다\n",
        "\n",
        "이 예측 확률에 로그 함수를 적용하면 더 좋다. 예측 확률 범위는 0 ~ 1인데, 이 예측 확률 범위의 값에 로그를 취하면 음수가 된다. 이렇게 되면 최종 손실 함수는 양수가 되므로 손실에 대해 이해하기 쉽고, 또한 로그 함수는 0에 가까울 수록 매우 큰 음수가 되므로 손실을 극대화하여 모델에 큰 영향을 미칠 수 있다\n",
        "\n",
        "정리하면 타깃이 양성 클래스일 때 (타깃이 1일 때) 손실은 -log(예측 확률)이 되고, 음성 클래스일 때 손실은 -log(1 - 예측 확률)로 계산이 된다. 이 손실 함수를 **로지스틱 손실 함수(logistic loss function)** 또는 **이진 크로스엔트로피 손실 함수(binary cross-entrophy loss function)**이라 한다. 다중 분류에서의 손실 함수는**크로스엔트로피 손실 함수(cross-entrophy loss function)**라고 한다\n",
        "\n",
        "분류가 아닌 회귀에서의 손실 함수는 보통 타깃에서 예측을 뺀 절댓값을 모든 샘플에 평균한 값인 평균 절댓값 오차를 사용하거나, 타깃에서 예측을 뺀 값을 제곱한 다음 모든 샘플에 평균한 값인 **평균 제곱 오차(mean squared error)**를 많이 사용한다\n",
        "\n",
        "## SGDClassifier\n",
        "아래에서 확률적 경사 하강법을 구현해보자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOkd2TusFH9j",
        "outputId": "94babf8a-773e-4b4f-f42b-25d1cc1f521f"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "fish = pd.read_csv('https://bit.ly/fish_csv_data')\n",
        "\n",
        "# Species 열을 제외한 나머지 5개는 입력 데이터로 사용, Species 열은 타깃 데이터\n",
        "fish_input = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy()\n",
        "fish_target = fish['Species'].to_numpy()\n",
        "\n",
        "# 데이터 훈련셋, 테스트셋으로 나누기\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_input, test_input, train_target, test_target = train_test_split(\n",
        "    fish_input, fish_target, random_state=42\n",
        ")\n",
        "\n",
        "# 훈련셋, 테스트셋 특성을 표준화 전처리\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "ss = StandardScaler()\n",
        "ss.fit(train_input)\n",
        "train_scaled = ss.transform(train_input)\n",
        "test_scaled = ss.transform(test_input)\n",
        "\n",
        "# SGD\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "# loss='log': 로지스틱 손실함수 지정, max_iter: 에폭 횟수 지정\n",
        "sc = SGDClassifier(loss='log', max_iter=10, random_state=42)\n",
        "sc.fit(train_scaled, train_target)\n",
        "print(sc.score(train_scaled, train_target))\n",
        "print(sc.score(test_scaled, test_target))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.773109243697479\n",
            "0.775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:700: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gljnqcjQUGPw",
        "outputId": "9227f457-b108-42ec-80a9-ebe431640c0c"
      },
      "source": [
        "# partial_fit으로 모델 이어서 훈련 가능\n",
        "sc.partial_fit(train_scaled, train_target)\n",
        "print(sc.score(train_scaled, train_target))\n",
        "print(sc.score(test_scaled, test_target))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8151260504201681\n",
            "0.825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfmOeBa-UcWE"
      },
      "source": [
        "## 에폭과 오버/언더피팅\n",
        "에폭이 적으면 모델이 훈련셋을 덜 학습할 수 있다. 에폭 횟수가 적절히 많아야 훈련셋을 완전히 학습할 수 있다. 즉 적은 에폭 횟수 동안에 훈련한 모델은 언더피팅일 가능성이 높고, 많은 에폭 횟수 동안에는 테스트셋에는 점수가 떨어지는 오버피팅일 가능성이 높다. 에폭이 진행될 수록 훈련셋의 점수는 꾸준히 증가하고, 테스트셋의 점수는 어느 순간 감소하기 시작하는데, 이 시점이 모델이 오버피팅되기 시작하는 곳이고 이것이 시작되기 전에 훈련을 멈추는 것을 **조기 종료(early stopping)**라 한다. 이에 관련된 그래프를 그려보자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "tTyTM2mLUPWo",
        "outputId": "c067d012-52c2-4b4c-9e84-2847e0987824"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "sc = SGDClassifier(loss='log', random_state=42)\n",
        "train_score = []\n",
        "test_score = []\n",
        "classes = np.unique(train_target)\n",
        "\n",
        "# 300번 에폭 반복\n",
        "for _ in range(0, 300):\n",
        "    sc.partial_fit(train_scaled, train_target, classes=classes)\n",
        "    train_score.append(sc.score(train_scaled, train_target))\n",
        "    test_score.append(sc.score(test_scaled, test_target))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_score)\n",
        "plt.plot(test_score)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sPLj0mFWQWm",
        "outputId": "ac96e3d6-4ee7-45dc-eb42-ad65eb7cf5d0"
      },
      "source": [
        "# 100번째 에폭이 적절하다 판단\n",
        "# tol=None으로 지정함으로써 조기종료 하지 않고 무조건 100 에폭 돌도록\n",
        "sc = SGDClassifier(loss='log', max_iter=100, tol=None, random_state=42)\n",
        "sc.fit(train_scaled, train_target)\n",
        "print(sc.score(train_scaled, train_target))\n",
        "print(sc.score(test_scaled, test_target))\n",
        "\n",
        "# 참고: loss 매개변수의 기본 값은 'hinge'이다. 힌지 손실(hinge loss)은\n",
        "# 서포트 벡터 머신이라 불리는 또 다른 머신러닝 알고리즘을 위한 손실함수다"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.957983193277311\n",
            "0.925\n"
          ]
        }
      ]
    }
  ]
}